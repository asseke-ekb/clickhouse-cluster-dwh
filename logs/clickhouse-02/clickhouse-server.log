2025.09.30 15:16:58.645199 [ 59 ] {} <Information> SentryWriter: Sending crash reports is disabled
2025.09.30 15:16:58.679007 [ 59 ] {} <Information> Application: Starting ClickHouse 24.3.18.7 (revision: 54496, git hash: 807f69cd6a86b00b05d369a9cd49c4c4e7a87788, build id: 01E72825FF490B6562012A8D711A33F324F30420), PID 59
2025.09.30 15:16:58.679293 [ 59 ] {} <Information> Application: starting up
2025.09.30 15:16:58.679450 [ 59 ] {} <Information> Application: OS name: Linux, version: 5.15.167.4-microsoft-standard-WSL2, architecture: x86_64
2025.09.30 15:16:58.683026 [ 59 ] {} <Information> Application: Available RAM: 15.47 GiB; logical cores: 20; used cores: 20.
2025.09.30 15:16:58.683462 [ 59 ] {} <Information> Application: Available CPU instruction sets: SSE, SSE2, SSE3, SSSE3, SSE41, SSE42, F16C, POPCNT, BMI1, BMI2, PCLMUL, AES, AVX, FMA, AVX2, SHA, ADX, RDRAND, RDSEED, RDTSCP, CLFLUSHOPT, CLWB, XSAVE, OSXSAVE
2025.09.30 15:16:58.684207 [ 59 ] {} <Information> Application: Shutting down storages.
2025.09.30 15:16:58.684420 [ 59 ] {} <Information> Application: Waiting for background threads
2025.09.30 15:16:58.684613 [ 59 ] {} <Information> Application: Background threads finished in 0 ms
2025.09.30 15:16:58.686110 [ 59 ] {} <Error> Application: Code: 137. DB::Exception: A setting 'max_threads' appeared at top level in config /etc/clickhouse-server/config.xml. But it is user-level setting that should be located in users.xml inside <profiles> section for specific profile. You can add it to <profiles><default> if you want to change default value of this setting. You can also disable the check - specify <skip_check_for_incorrect_settings>1</skip_check_for_incorrect_settings> in the main configuration file. (UNKNOWN_ELEMENT_IN_CONFIG), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception<String const&, String const&>(int, FormatStringHelperImpl<std::type_identity<String const&>::type, std::type_identity<String const&>::type>, String const&, String const&) @ 0x000000000766121d
2. DB::Settings::checkNoSettingNamesAtTopLevel(Poco::Util::AbstractConfiguration const&, String const&) @ 0x000000000fb420a2
3. DB::Server::main(std::vector<String, std::allocator<String>> const&) @ 0x000000000ca22f2a
4. Poco::Util::Application::run() @ 0x000000001499f706
5. DB::Server::run() @ 0x000000000ca1ec11
6. Poco::Util::ServerApplication::run(int, char**) @ 0x00000000149a85b9
7. mainEntryClickHouseServer(int, char**) @ 0x000000000ca1aa0a
8. main @ 0x0000000007656578
9. ? @ 0x00007f0758fc1083
10. _start @ 0x0000000005de262e
 (version 24.3.18.7 (official build))
2025.09.30 15:16:58.686551 [ 59 ] {} <Information> Application: shutting down
2025.09.30 15:16:58.688113 [ 62 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.09.30 15:26:08.452938 [ 59 ] {} <Information> SentryWriter: Sending crash reports is disabled
2025.09.30 15:26:08.485991 [ 59 ] {} <Information> Application: Starting ClickHouse 24.3.18.7 (revision: 54496, git hash: 807f69cd6a86b00b05d369a9cd49c4c4e7a87788, build id: 01E72825FF490B6562012A8D711A33F324F30420), PID 59
2025.09.30 15:26:08.486307 [ 59 ] {} <Information> Application: starting up
2025.09.30 15:26:08.486422 [ 59 ] {} <Information> Application: OS name: Linux, version: 5.15.167.4-microsoft-standard-WSL2, architecture: x86_64
2025.09.30 15:26:08.489272 [ 59 ] {} <Information> Application: Available RAM: 15.47 GiB; logical cores: 20; used cores: 20.
2025.09.30 15:26:08.489488 [ 59 ] {} <Information> Application: Available CPU instruction sets: SSE, SSE2, SSE3, SSSE3, SSE41, SSE42, F16C, POPCNT, BMI1, BMI2, PCLMUL, AES, AVX, FMA, AVX2, SHA, ADX, RDRAND, RDSEED, RDTSCP, CLFLUSHOPT, CLWB, XSAVE, OSXSAVE
2025.09.30 15:26:08.492487 [ 59 ] {} <Warning> Context: Linux transparent hugepages are set to "always". Check /sys/kernel/mm/transparent_hugepage/enabled
2025.09.30 15:26:08.493055 [ 59 ] {} <Warning> Context: Delay accounting is not enabled, OSIOWaitMicroseconds will not be gathered. You can enable it using `echo 1 > /proc/sys/kernel/task_delayacct` or by using sysctl.
2025.09.30 15:26:08.584519 [ 59 ] {} <Information> Application: Integrity check of the executable successfully passed (checksum: 320B53F8BF4FE8E8384861CB571F5F2D)
2025.09.30 15:26:08.584741 [ 59 ] {} <Information> Application: It looks like the process has no CAP_IPC_LOCK capability, binary mlock will be disabled. It could happen due to incorrect ClickHouse package installation. You could resolve the problem manually with 'sudo setcap cap_ipc_lock=+ep /usr/bin/clickhouse'. Note that it will not work on 'nosuid' mounted filesystems.
2025.09.30 15:26:08.588343 [ 59 ] {} <Information> Application: Lowered uncompressed cache size to 7.74 GiB because the system has limited RAM
2025.09.30 15:26:08.588486 [ 59 ] {} <Information> Application: Lowered mark cache size to 7.74 GiB because the system has limited RAM
2025.09.30 15:26:08.588732 [ 59 ] {} <Information> CgroupsMemoryUsageObserver: Will read the current memory usage from '/sys/fs/cgroup/memory' (cgroups version: v1), wait time is 15 sec
2025.09.30 15:26:08.599097 [ 59 ] {} <Information> Application: Setting max_server_memory_usage was set to 13.15 GiB (15.47 GiB available * 0.85 max_server_memory_usage_to_ram_ratio)
2025.09.30 15:26:08.599329 [ 59 ] {} <Information> CgroupsMemoryUsageObserver: Set new limits, soft limit: 11.84 GiB, hard limit: 12.50 GiB
2025.09.30 15:26:08.599440 [ 59 ] {} <Information> Application: Setting merges_mutations_memory_usage_soft_limit was set to 7.74 GiB (15.47 GiB available * 0.50 merges_mutations_memory_usage_to_ram_ratio)
2025.09.30 15:26:08.599537 [ 59 ] {} <Information> Application: Merges and mutations memory limit is set to 7.74 GiB
2025.09.30 15:26:08.600655 [ 59 ] {} <Information> BackgroundSchedulePool/BgBufSchPool: Create BackgroundSchedulePool with 16 threads
2025.09.30 15:26:08.605005 [ 59 ] {} <Information> BackgroundSchedulePool/BgSchPool: Create BackgroundSchedulePool with 64 threads
2025.09.30 15:26:08.616383 [ 59 ] {} <Information> BackgroundSchedulePool/BgMBSchPool: Create BackgroundSchedulePool with 8 threads
2025.09.30 15:26:08.619141 [ 59 ] {} <Information> BackgroundSchedulePool/BgDistSchPool: Create BackgroundSchedulePool with 16 threads
2025.09.30 15:26:08.624866 [ 59 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.09.30 15:26:08.625281 [ 59 ] {} <Information> Application: Listening for replica communication (interserver): http://127.0.0.1:9009
2025.09.30 15:26:08.631091 [ 59 ] {} <Warning> Access(local_directory): File /var/lib/clickhouse/access/users.list doesn't exist
2025.09.30 15:26:08.631411 [ 59 ] {} <Warning> Access(local_directory): Recovering lists in directory /var/lib/clickhouse/access/
2025.09.30 15:26:08.631765 [ 59 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.09.30 15:26:08.631894 [ 175 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 15.47 GiB
2025.09.30 15:26:08.634853 [ 59 ] {} <Information> Context: Initialized background executor for merges and mutations with num_threads=16, num_tasks=32, scheduling_policy=round_robin
2025.09.30 15:26:08.635464 [ 59 ] {} <Information> Context: Initialized background executor for move operations with num_threads=8, num_tasks=8
2025.09.30 15:26:08.638455 [ 59 ] {} <Information> Context: Initialized background executor for fetches with num_threads=16, num_tasks=16
2025.09.30 15:26:08.641377 [ 59 ] {} <Information> Context: Initialized background executor for common operations (e.g. clearing old parts) with num_threads=8, num_tasks=8
2025.09.30 15:26:08.642497 [ 59 ] {} <Information> DNSCacheUpdater: Update period 15 seconds
2025.09.30 15:26:08.642634 [ 59 ] {} <Information> Application: Loading metadata from /var/lib/clickhouse/
2025.09.30 15:26:08.653096 [ 59 ] {} <Information> DatabaseAtomic (system): Metadata processed, database system has 0 tables and 0 dictionaries in total.
2025.09.30 15:26:08.653297 [ 59 ] {} <Information> TablesLoader: Parsed metadata of 0 tables in 1 databases in 0.000361727 sec
2025.09.30 15:26:08.670385 [ 59 ] {} <Information> DatabaseCatalog: Found 0 partially dropped tables. Will load them and retry removal.
2025.09.30 15:26:08.674281 [ 59 ] {} <Information> DatabaseAtomic (default): Metadata processed, database default has 0 tables and 0 dictionaries in total.
2025.09.30 15:26:08.674463 [ 59 ] {} <Information> TablesLoader: Parsed metadata of 0 tables in 1 databases in 0.000242775 sec
2025.09.30 15:26:08.674675 [ 59 ] {} <Information> loadMetadata: Start synchronous loading of databases
2025.09.30 15:26:08.674834 [ 59 ] {} <Information> loadMetadata: Start synchronous startup of databases
2025.09.30 15:26:08.675841 [ 59 ] {} <Information> UserDefinedSQLObjectsLoaderFromDisk: Loading user defined objects from /var/lib/clickhouse/user_defined/
2025.09.30 15:26:08.676051 [ 59 ] {} <Information> Application: Tasks stats provider: procfs
2025.09.30 15:26:08.676152 [ 59 ] {} <Information> Application: It looks like the process has no CAP_SYS_NICE capability, the setting 'os_thread_priority' will have no effect. It could happen due to incorrect ClickHouse package installation. You could resolve the problem manually with 'sudo setcap cap_sys_nice=+ep /usr/bin/clickhouse'. Note that it will not work on 'nosuid' mounted filesystems.
2025.09.30 15:26:08.726232 [ 59 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.09.30 15:26:08.731746 [ 59 ] {} <Information> Application: Listening for http://127.0.0.1:8123
2025.09.30 15:26:08.731987 [ 59 ] {} <Information> Application: Listening for native protocol (tcp): 127.0.0.1:9000
2025.09.30 15:26:08.732195 [ 59 ] {} <Information> Application: Listening for MySQL compatibility protocol: 127.0.0.1:9004
2025.09.30 15:26:08.733316 [ 59 ] {} <Information> Application: Listening for PostgreSQL compatibility protocol: 127.0.0.1:9005
2025.09.30 15:26:08.733685 [ 59 ] {} <Information> Application: Listening for Prometheus: http://127.0.0.1:9363
2025.09.30 15:26:08.741986 [ 264 ] {} <Information> ZooKeeperClient: Connected to ZooKeeper at 172.18.0.2:2181 with session_id 144115332922867712
2025.09.30 15:26:08.746808 [ 264 ] {} <Information> ZooKeeperClient: Keeper feature flag FILTERED_LIST: disabled
2025.09.30 15:26:08.746969 [ 264 ] {} <Information> ZooKeeperClient: Keeper feature flag MULTI_READ: disabled
2025.09.30 15:26:08.747147 [ 264 ] {} <Information> ZooKeeperClient: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.09.30 15:26:08.747254 [ 264 ] {} <Information> ZooKeeperClient: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.09.30 15:26:08.762260 [ 59 ] {} <Information> Application: Ready for connections.
2025.09.30 15:26:09.443406 [ 62 ] {} <Information> Application: Received termination signal (Terminated)
2025.09.30 15:26:09.734944 [ 59 ] {} <Information> Application: Closed all listening sockets.
2025.09.30 15:26:09.735382 [ 59 ] {} <Information> Application: Closed connections.
2025.09.30 15:26:09.735618 [ 59 ] {} <Information> Application: Stopping AsyncLoader.
2025.09.30 15:26:09.736065 [ 59 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.09.30 15:26:09.738161 [ 59 ] {} <Information> Application: Shutting down storages.
2025.09.30 15:26:10.718021 [ 59 ] {} <Information> Context: Shutdown disk default
2025.09.30 15:26:10.718313 [ 59 ] {} <Information> ZooKeeperClient: Finalizing session 144115332922867712. finalization_started: false, queue_finished: false, reason: 'Destructor called'
2025.09.30 15:26:10.877915 [ 59 ] {} <Information> Application: Closed all listening sockets.
2025.09.30 15:26:10.878216 [ 59 ] {} <Information> Application: Closed connections to servers for tables.
2025.09.30 15:26:10.880130 [ 59 ] {} <Information> Application: Waiting for background threads
2025.09.30 15:26:10.887465 [ 59 ] {} <Information> Application: Background threads finished in 7 ms
2025.09.30 15:26:10.888112 [ 59 ] {} <Information> Application: shutting down
2025.09.30 15:26:10.888589 [ 62 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.09.30 15:26:10.930391 [ 1 ] {} <Information> SentryWriter: Sending crash reports is disabled
2025.09.30 15:26:10.960356 [ 1 ] {} <Information> Application: Starting ClickHouse 24.3.18.7 (revision: 54496, git hash: 807f69cd6a86b00b05d369a9cd49c4c4e7a87788, build id: 01E72825FF490B6562012A8D711A33F324F30420), PID 1
2025.09.30 15:26:10.961119 [ 1 ] {} <Information> Application: starting up
2025.09.30 15:26:10.961296 [ 1 ] {} <Information> Application: OS name: Linux, version: 5.15.167.4-microsoft-standard-WSL2, architecture: x86_64
2025.09.30 15:26:10.964140 [ 1 ] {} <Information> Application: Available RAM: 15.47 GiB; logical cores: 20; used cores: 20.
2025.09.30 15:26:10.964443 [ 1 ] {} <Information> Application: Available CPU instruction sets: SSE, SSE2, SSE3, SSSE3, SSE41, SSE42, F16C, POPCNT, BMI1, BMI2, PCLMUL, AES, AVX, FMA, AVX2, SHA, ADX, RDRAND, RDSEED, RDTSCP, CLFLUSHOPT, CLWB, XSAVE, OSXSAVE
2025.09.30 15:26:10.967017 [ 1 ] {} <Warning> Context: Linux transparent hugepages are set to "always". Check /sys/kernel/mm/transparent_hugepage/enabled
2025.09.30 15:26:10.968193 [ 1 ] {} <Warning> Context: Delay accounting is not enabled, OSIOWaitMicroseconds will not be gathered. You can enable it using `echo 1 > /proc/sys/kernel/task_delayacct` or by using sysctl.
2025.09.30 15:26:11.054126 [ 1 ] {} <Information> Application: Integrity check of the executable successfully passed (checksum: 320B53F8BF4FE8E8384861CB571F5F2D)
2025.09.30 15:26:11.054292 [ 1 ] {} <Information> Application: It looks like the process has no CAP_IPC_LOCK capability, binary mlock will be disabled. It could happen due to incorrect ClickHouse package installation. You could resolve the problem manually with 'sudo setcap cap_ipc_lock=+ep /usr/bin/clickhouse'. Note that it will not work on 'nosuid' mounted filesystems.
2025.09.30 15:26:11.054689 [ 1 ] {} <Information> Application: Lowered uncompressed cache size to 7.74 GiB because the system has limited RAM
2025.09.30 15:26:11.054779 [ 1 ] {} <Information> Application: Lowered mark cache size to 7.74 GiB because the system has limited RAM
2025.09.30 15:26:11.054981 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Will read the current memory usage from '/sys/fs/cgroup/memory' (cgroups version: v1), wait time is 15 sec
2025.09.30 15:26:11.065256 [ 1 ] {} <Information> Application: Setting max_server_memory_usage was set to 13.15 GiB (15.47 GiB available * 0.85 max_server_memory_usage_to_ram_ratio)
2025.09.30 15:26:11.065402 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Set new limits, soft limit: 11.84 GiB, hard limit: 12.50 GiB
2025.09.30 15:26:11.065525 [ 1 ] {} <Information> Application: Setting merges_mutations_memory_usage_soft_limit was set to 7.74 GiB (15.47 GiB available * 0.50 merges_mutations_memory_usage_to_ram_ratio)
2025.09.30 15:26:11.065639 [ 1 ] {} <Information> Application: Merges and mutations memory limit is set to 7.74 GiB
2025.09.30 15:26:11.066717 [ 1 ] {} <Information> BackgroundSchedulePool/BgBufSchPool: Create BackgroundSchedulePool with 16 threads
2025.09.30 15:26:11.068667 [ 1 ] {} <Information> BackgroundSchedulePool/BgSchPool: Create BackgroundSchedulePool with 64 threads
2025.09.30 15:26:11.079315 [ 1 ] {} <Information> BackgroundSchedulePool/BgMBSchPool: Create BackgroundSchedulePool with 8 threads
2025.09.30 15:26:11.080490 [ 1 ] {} <Information> BackgroundSchedulePool/BgDistSchPool: Create BackgroundSchedulePool with 16 threads
2025.09.30 15:26:11.083744 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.09.30 15:26:11.085677 [ 1 ] {} <Information> Application: Listening for replica communication (interserver): http://0.0.0.0:9009
2025.09.30 15:26:11.091660 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.09.30 15:26:11.091763 [ 385 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 15.47 GiB
2025.09.30 15:26:11.094621 [ 1 ] {} <Information> Context: Initialized background executor for merges and mutations with num_threads=16, num_tasks=32, scheduling_policy=round_robin
2025.09.30 15:26:11.095227 [ 1 ] {} <Information> Context: Initialized background executor for move operations with num_threads=8, num_tasks=8
2025.09.30 15:26:11.096377 [ 1 ] {} <Information> Context: Initialized background executor for fetches with num_threads=16, num_tasks=16
2025.09.30 15:26:11.096976 [ 1 ] {} <Information> Context: Initialized background executor for common operations (e.g. clearing old parts) with num_threads=8, num_tasks=8
2025.09.30 15:26:11.098247 [ 1 ] {} <Information> DNSCacheUpdater: Update period 15 seconds
2025.09.30 15:26:11.098375 [ 1 ] {} <Information> Application: Loading metadata from /var/lib/clickhouse/
2025.09.30 15:26:11.104797 [ 1 ] {} <Information> DatabaseAtomic (system): Metadata processed, database system has 4 tables and 0 dictionaries in total.
2025.09.30 15:26:11.104945 [ 1 ] {} <Information> TablesLoader: Parsed metadata of 4 tables in 1 databases in 0.006314023 sec
2025.09.30 15:26:11.128705 [ 1 ] {} <Information> DatabaseCatalog: Found 0 partially dropped tables. Will load them and retry removal.
2025.09.30 15:26:11.128955 [ 1 ] {} <Information> DatabaseAtomic (default): Metadata processed, database default has 0 tables and 0 dictionaries in total.
2025.09.30 15:26:11.129088 [ 1 ] {} <Information> TablesLoader: Parsed metadata of 0 tables in 1 databases in 0.000145903 sec
2025.09.30 15:26:11.129294 [ 1 ] {} <Information> loadMetadata: Start synchronous loading of databases
2025.09.30 15:26:11.129418 [ 1 ] {} <Information> loadMetadata: Start synchronous startup of databases
2025.09.30 15:26:11.129893 [ 1 ] {} <Information> UserDefinedSQLObjectsLoaderFromDisk: Loading user defined objects from /var/lib/clickhouse/user_defined/
2025.09.30 15:26:11.130100 [ 1 ] {} <Information> Application: Tasks stats provider: procfs
2025.09.30 15:26:11.130222 [ 1 ] {} <Information> Application: It looks like the process has no CAP_SYS_NICE capability, the setting 'os_thread_priority' will have no effect. It could happen due to incorrect ClickHouse package installation. You could resolve the problem manually with 'sudo setcap cap_sys_nice=+ep /usr/bin/clickhouse'. Note that it will not work on 'nosuid' mounted filesystems.
2025.09.30 15:26:11.177931 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.09.30 15:26:11.180320 [ 1 ] {} <Information> Application: Listening for http://0.0.0.0:8123
2025.09.30 15:26:11.180506 [ 1 ] {} <Information> Application: Listening for native protocol (tcp): 0.0.0.0:9000
2025.09.30 15:26:11.180650 [ 1 ] {} <Information> Application: Listening for MySQL compatibility protocol: 0.0.0.0:9004
2025.09.30 15:26:11.180784 [ 1 ] {} <Information> Application: Listening for PostgreSQL compatibility protocol: 0.0.0.0:9005
2025.09.30 15:26:11.180912 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.09.30 15:26:11.185915 [ 473 ] {} <Information> ZooKeeperClient: Connected to ZooKeeper at 172.18.0.2:2181 with session_id 144115332922867713
2025.09.30 15:26:11.188189 [ 473 ] {} <Information> ZooKeeperClient: Keeper feature flag FILTERED_LIST: disabled
2025.09.30 15:26:11.188299 [ 473 ] {} <Information> ZooKeeperClient: Keeper feature flag MULTI_READ: disabled
2025.09.30 15:26:11.188388 [ 473 ] {} <Information> ZooKeeperClient: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.09.30 15:26:11.188473 [ 473 ] {} <Information> ZooKeeperClient: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.09.30 15:26:11.193055 [ 1 ] {} <Information> Application: Ready for connections.
2025.09.30 15:27:53.464992 [ 275 ] {} <Error> Access(user directories): from: 172.18.0.1, user: admin: Authentication failed: Code: 193. DB::Exception: Invalid credentials. (WRONG_PASSWORD), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception<>(int, FormatStringHelperImpl<>) @ 0x00000000076664e3
2. DB::IAccessStorage::throwInvalidCredentials() @ 0x000000000f889258
3. DB::IAccessStorage::authenticateImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool, bool, bool) const @ 0x000000000f888e61
4. DB::MultipleAccessStorage::authenticateImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool, bool, bool) const @ 0x000000000f8c003a
5. DB::AccessControl::authenticate(DB::Credentials const&, Poco::Net::IPAddress const&, String const&) const @ 0x000000000f7fb8a5
6. DB::Session::authenticate(DB::Credentials const&, Poco::Net::SocketAddress const&) @ 0x0000000010cd3223
7. DB::HTTPHandler::authenticateUser(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&) @ 0x0000000011fda064
8. DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::optional<DB::CurrentThread::QueryScope>&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fdb87e
9. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fe3e34
10. DB::HTTPServerConnection::run() @ 0x00000000120634da
11. Poco::Net::TCPServerConnection::start() @ 0x00000000149929d2
12. Poco::Net::TCPServerDispatcher::run() @ 0x0000000014993819
13. Poco::PooledThread::run() @ 0x0000000014a8bf81
14. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000014a8a51d
15. ? @ 0x00007f5f0887e609
16. ? @ 0x00007f5f087a3353
 (version 24.3.18.7 (official build))
2025.09.30 15:27:53.465611 [ 275 ] {} <Error> DynamicQueryHandler: Code: 516. DB::Exception: admin: Authentication failed: password is incorrect, or there is no user with such name. (AUTHENTICATION_FAILED), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x000000000774a4cc
2. DB::AccessControl::authenticate(DB::Credentials const&, Poco::Net::IPAddress const&, String const&) const @ 0x000000000f7fbd6b
3. DB::Session::authenticate(DB::Credentials const&, Poco::Net::SocketAddress const&) @ 0x0000000010cd3223
4. DB::HTTPHandler::authenticateUser(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&) @ 0x0000000011fda064
5. DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::optional<DB::CurrentThread::QueryScope>&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fdb87e
6. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fe3e34
7. DB::HTTPServerConnection::run() @ 0x00000000120634da
8. Poco::Net::TCPServerConnection::start() @ 0x00000000149929d2
9. Poco::Net::TCPServerDispatcher::run() @ 0x0000000014993819
10. Poco::PooledThread::run() @ 0x0000000014a8bf81
11. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000014a8a51d
12. ? @ 0x00007f5f0887e609
13. ? @ 0x00007f5f087a3353
 (version 24.3.18.7 (official build))
2025.09.30 15:27:53.492684 [ 275 ] {} <Error> Access(user directories): from: 172.18.0.1, user: admin: Authentication failed: Code: 193. DB::Exception: Invalid credentials. (WRONG_PASSWORD), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception<>(int, FormatStringHelperImpl<>) @ 0x00000000076664e3
2. DB::IAccessStorage::throwInvalidCredentials() @ 0x000000000f889258
3. DB::IAccessStorage::authenticateImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool, bool, bool) const @ 0x000000000f888e61
4. DB::MultipleAccessStorage::authenticateImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool, bool, bool) const @ 0x000000000f8c003a
5. DB::AccessControl::authenticate(DB::Credentials const&, Poco::Net::IPAddress const&, String const&) const @ 0x000000000f7fb8a5
6. DB::Session::authenticate(DB::Credentials const&, Poco::Net::SocketAddress const&) @ 0x0000000010cd3223
7. DB::HTTPHandler::authenticateUser(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&) @ 0x0000000011fda064
8. DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::optional<DB::CurrentThread::QueryScope>&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fdb87e
9. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fe3e34
10. DB::HTTPServerConnection::run() @ 0x00000000120634da
11. Poco::Net::TCPServerConnection::start() @ 0x00000000149929d2
12. Poco::Net::TCPServerDispatcher::run() @ 0x0000000014993819
13. Poco::PooledThread::run() @ 0x0000000014a8bf81
14. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000014a8a51d
15. ? @ 0x00007f5f0887e609
16. ? @ 0x00007f5f087a3353
 (version 24.3.18.7 (official build))
2025.09.30 15:27:53.493851 [ 275 ] {} <Error> DynamicQueryHandler: Code: 516. DB::Exception: admin: Authentication failed: password is incorrect, or there is no user with such name. (AUTHENTICATION_FAILED), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x000000000774a4cc
2. DB::AccessControl::authenticate(DB::Credentials const&, Poco::Net::IPAddress const&, String const&) const @ 0x000000000f7fbd6b
3. DB::Session::authenticate(DB::Credentials const&, Poco::Net::SocketAddress const&) @ 0x0000000010cd3223
4. DB::HTTPHandler::authenticateUser(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&) @ 0x0000000011fda064
5. DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::optional<DB::CurrentThread::QueryScope>&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fdb87e
6. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fe3e34
7. DB::HTTPServerConnection::run() @ 0x00000000120634da
8. Poco::Net::TCPServerConnection::start() @ 0x00000000149929d2
9. Poco::Net::TCPServerDispatcher::run() @ 0x0000000014993819
10. Poco::PooledThread::run() @ 0x0000000014a8bf81
11. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000014a8a51d
12. ? @ 0x00007f5f0887e609
13. ? @ 0x00007f5f087a3353
 (version 24.3.18.7 (official build))
2025.09.30 15:38:31.213763 [ 275 ] {33cf92db-33c1-42d6-8758-e1853c42ee18} <Error> executeQuery: Code: 497. DB::Exception: etl: Not enough privileges. To execute this query, it's necessary to have the grant SHOW USERS ON *.*. (ACCESS_DENIED) (version 24.3.18.7 (official build)) (from 172.18.0.10:35372) (in query: show grants for analytics), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception<String, String>(int, FormatStringHelperImpl<std::type_identity<String>::type, std::type_identity<String>::type>, String&&, String&&) @ 0x000000000765b01d
2. bool DB::ContextAccess::checkAccessImplHelper<true, false>(DB::AccessFlags) const @ 0x000000000f83b02e
3. bool DB::ContextAccess::checkAccessImplHelper<true, false>(DB::AccessRightsElement const&) const @ 0x000000000f846753
4. DB::CachedAccessChecking::checkAccess(bool) @ 0x000000001107530a
5. DB::InterpreterShowGrantsQuery::execute() @ 0x0000000011070fa6
6. DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) @ 0x0000000010f4f7f8
7. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&)>) @ 0x0000000010f54d32
8. DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::optional<DB::CurrentThread::QueryScope>&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fdf375
9. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fe3e34
10. DB::HTTPServerConnection::run() @ 0x00000000120634da
11. Poco::Net::TCPServerConnection::start() @ 0x00000000149929d2
12. Poco::Net::TCPServerDispatcher::run() @ 0x0000000014993819
13. Poco::PooledThread::run() @ 0x0000000014a8bf81
14. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000014a8a51d
15. ? @ 0x00007f5f0887e609
16. ? @ 0x00007f5f087a3353

2025.09.30 15:38:31.214624 [ 275 ] {33cf92db-33c1-42d6-8758-e1853c42ee18} <Error> DynamicQueryHandler: Code: 497. DB::Exception: etl: Not enough privileges. To execute this query, it's necessary to have the grant SHOW USERS ON *.*. (ACCESS_DENIED), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception<String, String>(int, FormatStringHelperImpl<std::type_identity<String>::type, std::type_identity<String>::type>, String&&, String&&) @ 0x000000000765b01d
2. bool DB::ContextAccess::checkAccessImplHelper<true, false>(DB::AccessFlags) const @ 0x000000000f83b02e
3. bool DB::ContextAccess::checkAccessImplHelper<true, false>(DB::AccessRightsElement const&) const @ 0x000000000f846753
4. DB::CachedAccessChecking::checkAccess(bool) @ 0x000000001107530a
5. DB::InterpreterShowGrantsQuery::execute() @ 0x0000000011070fa6
6. DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) @ 0x0000000010f4f7f8
7. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&)>) @ 0x0000000010f54d32
8. DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::optional<DB::CurrentThread::QueryScope>&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fdf375
9. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x0000000011fe3e34
10. DB::HTTPServerConnection::run() @ 0x00000000120634da
11. Poco::Net::TCPServerConnection::start() @ 0x00000000149929d2
12. Poco::Net::TCPServerDispatcher::run() @ 0x0000000014993819
13. Poco::PooledThread::run() @ 0x0000000014a8bf81
14. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000014a8a51d
15. ? @ 0x00007f5f0887e609
16. ? @ 0x00007f5f087a3353
 (version 24.3.18.7 (official build))
2025.09.30 15:50:26.290892 [ 473 ] {d81611e4-d598-45ea-ac0f-79cfc74f3cd0} <Information> DatabaseAtomic (stage0): Metadata processed, database stage0 has 0 tables and 0 dictionaries in total.
2025.09.30 15:50:26.291230 [ 473 ] {d81611e4-d598-45ea-ac0f-79cfc74f3cd0} <Information> TablesLoader: Parsed metadata of 0 tables in 1 databases in 0.000391753 sec
2025.09.30 15:50:28.679473 [ 473 ] {7991a384-c229-43eb-bf86-92f43dcd94e9} <Information> DatabaseAtomic (stage1): Metadata processed, database stage1 has 0 tables and 0 dictionaries in total.
2025.09.30 15:50:28.680030 [ 473 ] {7991a384-c229-43eb-bf86-92f43dcd94e9} <Information> TablesLoader: Parsed metadata of 0 tables in 1 databases in 0.000622572 sec
2025.09.30 15:50:32.038200 [ 473 ] {0d617c69-8622-48ae-8a8d-8a8d7dd826cf} <Information> DatabaseAtomic (stage2): Metadata processed, database stage2 has 0 tables and 0 dictionaries in total.
2025.09.30 15:50:32.038530 [ 473 ] {0d617c69-8622-48ae-8a8d-8a8d7dd826cf} <Information> TablesLoader: Parsed metadata of 0 tables in 1 databases in 0.000395733 sec
2025.09.30 15:50:33.605194 [ 473 ] {732300f4-77e5-4e35-8027-377a0c01a47b} <Information> DatabaseAtomic (reports): Metadata processed, database reports has 0 tables and 0 dictionaries in total.
2025.09.30 15:50:33.605529 [ 473 ] {732300f4-77e5-4e35-8027-377a0c01a47b} <Information> TablesLoader: Parsed metadata of 0 tables in 1 databases in 0.00039839 sec
2025.09.30 15:51:28.389391 [ 473 ] {5b65de58-0774-4664-87ea-7db719c6568d} <Error> executeQuery: Code: 450. DB::Exception: TTL expression result column should have DateTime or Date type, but has DateTime64(3). (BAD_TTL_EXPRESSION) (version 24.3.18.7 (official build)) (from 0.0.0.0:0) (in query: /* ddl_entry=query-0000000004 */ CREATE TABLE IF NOT EXISTS stage0.orders_raw UUID 'c2739568-f5b8-4a99-9e48-50da2b4c9793' (`order_id` UInt64, `customer_id` UInt64, `order_date` DateTime, `total_amount` Decimal(18, 2), `status` LowCardinality(String), `_operation` LowCardinality(String), `_timestamp` DateTime64(3), `_source_timestamp` DateTime64(3), `_deleted` UInt8 DEFAULT 0, `_version` UInt64, `_kafka_topic` String, `_kafka_partition` UInt64, `_kafka_offset` UInt64) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/stage0/orders_raw', '{replica}', _version) PARTITION BY toYYYYMM(order_date) ORDER BY (order_id, _timestamp) TTL _timestamp + toIntervalDay(30) SETTINGS index_granularity = 8192, merge_with_ttl_timeout = 86400), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception<String>(int, FormatStringHelperImpl<std::type_identity<String>::type>, String&&) @ 0x0000000007659b23
2. DB::TTLDescription::getTTLFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x0000000011729396
3. DB::TTLTableDescription::getTTLForTableFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x000000001172a37e
4. DB::create(DB::StorageFactory::Arguments const&) @ 0x0000000011dda045
5. DB::StorageFactory::get(DB::ASTCreateQuery const&, String const&, std::shared_ptr<DB::Context>, std::shared_ptr<DB::Context>, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, DB::LoadingStrictnessLevel) const @ 0x00000000113e277d
6. DB::InterpreterCreateQuery::doCreateTable(DB::ASTCreateQuery&, DB::InterpreterCreateQuery::TableProperties const&, std::unique_ptr<DB::DDLGuard, std::default_delete<DB::DDLGuard>>&) @ 0x000000001085a2b6
7. DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x00000000108508c0
8. DB::InterpreterCreateQuery::execute() @ 0x000000001086168f
9. DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) @ 0x0000000010f4f7f8
10. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&)>) @ 0x0000000010f54d32
11. DB::DDLWorker::tryExecuteQuery(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010219e99
12. DB::DDLWorker::processTask(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010218155
13. DB::DDLWorker::scheduleTasks(bool) @ 0x0000000010214e73
14. DB::DDLWorker::runMainThread() @ 0x000000001020da4e
15. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x0000000010228b34
16. void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000c94f12d
17. ? @ 0x00007f5f0887e609
18. ? @ 0x00007f5f087a3353

2025.09.30 15:51:28.390076 [ 473 ] {5b65de58-0774-4664-87ea-7db719c6568d} <Error> DDLWorker: Query /* ddl_entry=query-0000000004 */ CREATE TABLE IF NOT EXISTS stage0.orders_raw UUID 'c2739568-f5b8-4a99-9e48-50da2b4c9793' (`order_id` UInt64, `customer_id` UInt64, `order_date` DateTime, `total_amount` Decimal(18, 2), `status` LowCardinality(String), `_operation` LowCardinality(String), `_timestamp` DateTime64(3), `_source_timestamp` DateTime64(3), `_deleted` UInt8 DEFAULT 0, `_version` UInt64, `_kafka_topic` String, `_kafka_partition` UInt64, `_kafka_offset` UInt64) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/stage0/orders_raw', '{replica}', _version) PARTITION BY toYYYYMM(order_date) ORDER BY (order_id, _timestamp) TTL _timestamp + toIntervalDay(30) SETTINGS index_granularity = 8192, merge_with_ttl_timeout = 86400 wasn't finished successfully: Code: 450. DB::Exception: TTL expression result column should have DateTime or Date type, but has DateTime64(3). (BAD_TTL_EXPRESSION), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception<String>(int, FormatStringHelperImpl<std::type_identity<String>::type>, String&&) @ 0x0000000007659b23
2. DB::TTLDescription::getTTLFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x0000000011729396
3. DB::TTLTableDescription::getTTLForTableFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x000000001172a37e
4. DB::create(DB::StorageFactory::Arguments const&) @ 0x0000000011dda045
5. DB::StorageFactory::get(DB::ASTCreateQuery const&, String const&, std::shared_ptr<DB::Context>, std::shared_ptr<DB::Context>, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, DB::LoadingStrictnessLevel) const @ 0x00000000113e277d
6. DB::InterpreterCreateQuery::doCreateTable(DB::ASTCreateQuery&, DB::InterpreterCreateQuery::TableProperties const&, std::unique_ptr<DB::DDLGuard, std::default_delete<DB::DDLGuard>>&) @ 0x000000001085a2b6
7. DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x00000000108508c0
8. DB::InterpreterCreateQuery::execute() @ 0x000000001086168f
9. DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) @ 0x0000000010f4f7f8
10. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&)>) @ 0x0000000010f54d32
11. DB::DDLWorker::tryExecuteQuery(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010219e99
12. DB::DDLWorker::processTask(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010218155
13. DB::DDLWorker::scheduleTasks(bool) @ 0x0000000010214e73
14. DB::DDLWorker::runMainThread() @ 0x000000001020da4e
15. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x0000000010228b34
16. void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000c94f12d
17. ? @ 0x00007f5f0887e609
18. ? @ 0x00007f5f087a3353
 (version 24.3.18.7 (official build))
2025.09.30 15:51:36.844606 [ 473 ] {0057989e-bc8f-4d25-bd43-f5c0b2a4e48e} <Error> executeQuery: Code: 450. DB::Exception: TTL expression result column should have DateTime or Date type, but has DateTime64(3). (BAD_TTL_EXPRESSION) (version 24.3.18.7 (official build)) (from 0.0.0.0:0) (in query: /* ddl_entry=query-0000000005 */ CREATE TABLE IF NOT EXISTS stage0.orders_raw UUID 'ad8669ff-1a15-4308-bb35-eabc637a86cc' (`order_id` UInt64, `customer_id` UInt64, `order_date` DateTime, `total_amount` Decimal(18, 2), `status` LowCardinality(String), `_operation` LowCardinality(String), `_timestamp` DateTime64(3), `_source_timestamp` DateTime64(3), `_deleted` UInt8 DEFAULT 0, `_version` UInt64, `_kafka_topic` String, `_kafka_partition` UInt64, `_kafka_offset` UInt64) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/stage0/orders_raw', '{replica}', _version) PARTITION BY toYYYYMM(order_date) ORDER BY (order_id, _timestamp) TTL _timestamp + toIntervalDay(30) SETTINGS index_granularity = 8192, merge_with_ttl_timeout = 86400), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception<String>(int, FormatStringHelperImpl<std::type_identity<String>::type>, String&&) @ 0x0000000007659b23
2. DB::TTLDescription::getTTLFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x0000000011729396
3. DB::TTLTableDescription::getTTLForTableFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x000000001172a37e
4. DB::create(DB::StorageFactory::Arguments const&) @ 0x0000000011dda045
5. DB::StorageFactory::get(DB::ASTCreateQuery const&, String const&, std::shared_ptr<DB::Context>, std::shared_ptr<DB::Context>, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, DB::LoadingStrictnessLevel) const @ 0x00000000113e277d
6. DB::InterpreterCreateQuery::doCreateTable(DB::ASTCreateQuery&, DB::InterpreterCreateQuery::TableProperties const&, std::unique_ptr<DB::DDLGuard, std::default_delete<DB::DDLGuard>>&) @ 0x000000001085a2b6
7. DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x00000000108508c0
8. DB::InterpreterCreateQuery::execute() @ 0x000000001086168f
9. DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) @ 0x0000000010f4f7f8
10. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&)>) @ 0x0000000010f54d32
11. DB::DDLWorker::tryExecuteQuery(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010219e99
12. DB::DDLWorker::processTask(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010218155
13. DB::DDLWorker::scheduleTasks(bool) @ 0x0000000010214e73
14. DB::DDLWorker::runMainThread() @ 0x000000001020da4e
15. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x0000000010228b34
16. void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000c94f12d
17. ? @ 0x00007f5f0887e609
18. ? @ 0x00007f5f087a3353

2025.09.30 15:51:36.844974 [ 473 ] {0057989e-bc8f-4d25-bd43-f5c0b2a4e48e} <Error> DDLWorker: Query /* ddl_entry=query-0000000005 */ CREATE TABLE IF NOT EXISTS stage0.orders_raw UUID 'ad8669ff-1a15-4308-bb35-eabc637a86cc' (`order_id` UInt64, `customer_id` UInt64, `order_date` DateTime, `total_amount` Decimal(18, 2), `status` LowCardinality(String), `_operation` LowCardinality(String), `_timestamp` DateTime64(3), `_source_timestamp` DateTime64(3), `_deleted` UInt8 DEFAULT 0, `_version` UInt64, `_kafka_topic` String, `_kafka_partition` UInt64, `_kafka_offset` UInt64) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/stage0/orders_raw', '{replica}', _version) PARTITION BY toYYYYMM(order_date) ORDER BY (order_id, _timestamp) TTL _timestamp + toIntervalDay(30) SETTINGS index_granularity = 8192, merge_with_ttl_timeout = 86400 wasn't finished successfully: Code: 450. DB::Exception: TTL expression result column should have DateTime or Date type, but has DateTime64(3). (BAD_TTL_EXPRESSION), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception<String>(int, FormatStringHelperImpl<std::type_identity<String>::type>, String&&) @ 0x0000000007659b23
2. DB::TTLDescription::getTTLFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x0000000011729396
3. DB::TTLTableDescription::getTTLForTableFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x000000001172a37e
4. DB::create(DB::StorageFactory::Arguments const&) @ 0x0000000011dda045
5. DB::StorageFactory::get(DB::ASTCreateQuery const&, String const&, std::shared_ptr<DB::Context>, std::shared_ptr<DB::Context>, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, DB::LoadingStrictnessLevel) const @ 0x00000000113e277d
6. DB::InterpreterCreateQuery::doCreateTable(DB::ASTCreateQuery&, DB::InterpreterCreateQuery::TableProperties const&, std::unique_ptr<DB::DDLGuard, std::default_delete<DB::DDLGuard>>&) @ 0x000000001085a2b6
7. DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x00000000108508c0
8. DB::InterpreterCreateQuery::execute() @ 0x000000001086168f
9. DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) @ 0x0000000010f4f7f8
10. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&)>) @ 0x0000000010f54d32
11. DB::DDLWorker::tryExecuteQuery(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010219e99
12. DB::DDLWorker::processTask(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010218155
13. DB::DDLWorker::scheduleTasks(bool) @ 0x0000000010214e73
14. DB::DDLWorker::runMainThread() @ 0x000000001020da4e
15. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x0000000010228b34
16. void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000c94f12d
17. ? @ 0x00007f5f0887e609
18. ? @ 0x00007f5f087a3353
 (version 24.3.18.7 (official build))
2025.09.30 15:52:07.572615 [ 473 ] {569b29d1-38ac-490d-b085-918996555466} <Error> executeQuery: Code: 450. DB::Exception: TTL expression result column should have DateTime or Date type, but has DateTime64(3). (BAD_TTL_EXPRESSION) (version 24.3.18.7 (official build)) (from 0.0.0.0:0) (in query: /* ddl_entry=query-0000000006 */ CREATE TABLE IF NOT EXISTS stage0.orders_raw UUID '74784f13-4549-491c-83d9-5a40788c91dc' (`order_id` UInt64, `customer_id` UInt64, `order_date` DateTime64(3), `total_amount` Decimal(18, 2), `status` LowCardinality(String), `_operation` LowCardinality(String), `_timestamp` DateTime64(3), `_source_timestamp` DateTime64(3), `_deleted` UInt8 DEFAULT 0, `_version` UInt64, `_kafka_topic` String, `_kafka_partition` UInt64, `_kafka_offset` UInt64) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/stage0/orders_raw', '{replica}', _version) PARTITION BY toYYYYMM(order_date) ORDER BY (order_id, _timestamp) TTL _timestamp + toIntervalDay(30) SETTINGS index_granularity = 8192, merge_with_ttl_timeout = 86400), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception<String>(int, FormatStringHelperImpl<std::type_identity<String>::type>, String&&) @ 0x0000000007659b23
2. DB::TTLDescription::getTTLFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x0000000011729396
3. DB::TTLTableDescription::getTTLForTableFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x000000001172a37e
4. DB::create(DB::StorageFactory::Arguments const&) @ 0x0000000011dda045
5. DB::StorageFactory::get(DB::ASTCreateQuery const&, String const&, std::shared_ptr<DB::Context>, std::shared_ptr<DB::Context>, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, DB::LoadingStrictnessLevel) const @ 0x00000000113e277d
6. DB::InterpreterCreateQuery::doCreateTable(DB::ASTCreateQuery&, DB::InterpreterCreateQuery::TableProperties const&, std::unique_ptr<DB::DDLGuard, std::default_delete<DB::DDLGuard>>&) @ 0x000000001085a2b6
7. DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x00000000108508c0
8. DB::InterpreterCreateQuery::execute() @ 0x000000001086168f
9. DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) @ 0x0000000010f4f7f8
10. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&)>) @ 0x0000000010f54d32
11. DB::DDLWorker::tryExecuteQuery(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010219e99
12. DB::DDLWorker::processTask(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010218155
13. DB::DDLWorker::scheduleTasks(bool) @ 0x0000000010214e73
14. DB::DDLWorker::runMainThread() @ 0x000000001020da4e
15. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x0000000010228b34
16. void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000c94f12d
17. ? @ 0x00007f5f0887e609
18. ? @ 0x00007f5f087a3353

2025.09.30 15:52:07.573114 [ 473 ] {569b29d1-38ac-490d-b085-918996555466} <Error> DDLWorker: Query /* ddl_entry=query-0000000006 */ CREATE TABLE IF NOT EXISTS stage0.orders_raw UUID '74784f13-4549-491c-83d9-5a40788c91dc' (`order_id` UInt64, `customer_id` UInt64, `order_date` DateTime64(3), `total_amount` Decimal(18, 2), `status` LowCardinality(String), `_operation` LowCardinality(String), `_timestamp` DateTime64(3), `_source_timestamp` DateTime64(3), `_deleted` UInt8 DEFAULT 0, `_version` UInt64, `_kafka_topic` String, `_kafka_partition` UInt64, `_kafka_offset` UInt64) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/stage0/orders_raw', '{replica}', _version) PARTITION BY toYYYYMM(order_date) ORDER BY (order_id, _timestamp) TTL _timestamp + toIntervalDay(30) SETTINGS index_granularity = 8192, merge_with_ttl_timeout = 86400 wasn't finished successfully: Code: 450. DB::Exception: TTL expression result column should have DateTime or Date type, but has DateTime64(3). (BAD_TTL_EXPRESSION), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000c8a00db
1. DB::Exception::Exception<String>(int, FormatStringHelperImpl<std::type_identity<String>::type>, String&&) @ 0x0000000007659b23
2. DB::TTLDescription::getTTLFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x0000000011729396
3. DB::TTLTableDescription::getTTLForTableFromAST(std::shared_ptr<DB::IAST> const&, DB::ColumnsDescription const&, std::shared_ptr<DB::Context const>, DB::KeyDescription const&, bool) @ 0x000000001172a37e
4. DB::create(DB::StorageFactory::Arguments const&) @ 0x0000000011dda045
5. DB::StorageFactory::get(DB::ASTCreateQuery const&, String const&, std::shared_ptr<DB::Context>, std::shared_ptr<DB::Context>, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, DB::LoadingStrictnessLevel) const @ 0x00000000113e277d
6. DB::InterpreterCreateQuery::doCreateTable(DB::ASTCreateQuery&, DB::InterpreterCreateQuery::TableProperties const&, std::unique_ptr<DB::DDLGuard, std::default_delete<DB::DDLGuard>>&) @ 0x000000001085a2b6
7. DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x00000000108508c0
8. DB::InterpreterCreateQuery::execute() @ 0x000000001086168f
9. DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) @ 0x0000000010f4f7f8
10. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&)>) @ 0x0000000010f54d32
11. DB::DDLWorker::tryExecuteQuery(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010219e99
12. DB::DDLWorker::processTask(DB::DDLTaskBase&, std::shared_ptr<zkutil::ZooKeeper> const&) @ 0x0000000010218155
13. DB::DDLWorker::scheduleTasks(bool) @ 0x0000000010214e73
14. DB::DDLWorker::runMainThread() @ 0x000000001020da4e
15. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x0000000010228b34
16. void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000c94f12d
17. ? @ 0x00007f5f0887e609
18. ? @ 0x00007f5f087a3353
 (version 24.3.18.7 (official build))
2025.09.30 15:52:53.851636 [ 473 ] {dd872a0a-47ff-4486-9430-3f8790024a27} <Information> stage0.orders_raw (f87b21bb-d797-426c-a073-cac328033e63): It looks like the table /clickhouse/tables/01/stage0/orders_raw was created by another server at the same moment, will retry
2025.09.30 15:52:53.865263 [ 473 ] {dd872a0a-47ff-4486-9430-3f8790024a27} <Information> stage0.orders_raw (f87b21bb-d797-426c-a073-cac328033e63): Retrying createReplica(), because some other replicas were created at the same time
2025.09.30 15:52:53.897196 [ 473 ] {dd872a0a-47ff-4486-9430-3f8790024a27} <Information> stage0.orders_raw (f87b21bb-d797-426c-a073-cac328033e63): Became leader
2025.09.30 15:52:53.904143 [ 314 ] {} <Information> stage0.orders_raw (f87b21bb-d797-426c-a073-cac328033e63): Not cloning replica_03, it's lost
2025.09.30 15:52:53.904284 [ 314 ] {} <Information> stage0.orders_raw (f87b21bb-d797-426c-a073-cac328033e63): Replica replica_01 has log pointer '', approximate 1 queue lag and 0 queue size
2025.09.30 15:52:53.904422 [ 314 ] {} <Information> stage0.orders_raw (f87b21bb-d797-426c-a073-cac328033e63): Will mimic replica_01
2025.09.30 15:52:58.245516 [ 473 ] {e916f91a-7840-46b0-a3bd-5b99c3bb874b} <Information> stage1.`.inner_id.7758e2f0-8542-4a40-b203-ea11a1f71a2b` (51e02938-c344-4807-9dba-2d623097935c): Became leader
2025.09.30 15:53:03.237557 [ 473 ] {98328d26-de61-4cc5-9de4-db4006abaf15} <Information> stage2.`.inner_id.72cd51d1-c1fa-4bb6-be0a-07c29e4c686b` (c828aa04-b4c4-4eef-9b4a-4efeb3b2de02): It looks like the table /clickhouse/tables/01/stage2/daily_sales_summary was created by another server at the same moment, will retry
2025.09.30 15:53:03.293729 [ 473 ] {98328d26-de61-4cc5-9de4-db4006abaf15} <Information> stage2.`.inner_id.72cd51d1-c1fa-4bb6-be0a-07c29e4c686b` (c828aa04-b4c4-4eef-9b4a-4efeb3b2de02): LeaderElection: leader suddenly changed or new node appeared, will retry
2025.09.30 15:53:03.294393 [ 473 ] {98328d26-de61-4cc5-9de4-db4006abaf15} <Information> stage2.`.inner_id.72cd51d1-c1fa-4bb6-be0a-07c29e4c686b` (c828aa04-b4c4-4eef-9b4a-4efeb3b2de02): Became leader
2025.09.30 15:53:03.303149 [ 373 ] {} <Information> stage2.`.inner_id.72cd51d1-c1fa-4bb6-be0a-07c29e4c686b` (c828aa04-b4c4-4eef-9b4a-4efeb3b2de02): Replica replica_03 has log pointer '', approximate 1 queue lag and 0 queue size
2025.09.30 15:53:03.303552 [ 373 ] {} <Information> stage2.`.inner_id.72cd51d1-c1fa-4bb6-be0a-07c29e4c686b` (c828aa04-b4c4-4eef-9b4a-4efeb3b2de02): Not cloning replica_01, it's lost
2025.09.30 15:53:03.303860 [ 373 ] {} <Information> stage2.`.inner_id.72cd51d1-c1fa-4bb6-be0a-07c29e4c686b` (c828aa04-b4c4-4eef-9b4a-4efeb3b2de02): Will mimic replica_03
2025.09.30 15:53:18.533641 [ 473 ] {8adda0fd-e5ab-4c98-97ec-c9b26f77bfed} <Information> reports.monthly_business_metrics (b3f1459f-871d-4922-be9d-193684354eae): It looks like the table /clickhouse/tables/01/reports/monthly_business_metrics was created by another server at the same moment, will retry
2025.09.30 15:53:18.542546 [ 473 ] {8adda0fd-e5ab-4c98-97ec-c9b26f77bfed} <Information> reports.monthly_business_metrics (b3f1459f-871d-4922-be9d-193684354eae): Retrying createReplica(), because some other replicas were created at the same time
2025.09.30 15:53:18.562018 [ 473 ] {8adda0fd-e5ab-4c98-97ec-c9b26f77bfed} <Information> reports.monthly_business_metrics (b3f1459f-871d-4922-be9d-193684354eae): Became leader
2025.09.30 15:53:18.568206 [ 323 ] {} <Information> reports.monthly_business_metrics (b3f1459f-871d-4922-be9d-193684354eae): Replica replica_03 has log pointer '', approximate 1 queue lag and 0 queue size
2025.09.30 15:53:18.568356 [ 323 ] {} <Information> reports.monthly_business_metrics (b3f1459f-871d-4922-be9d-193684354eae): Not cloning replica_01, it's lost
2025.09.30 15:53:18.568508 [ 323 ] {} <Information> reports.monthly_business_metrics (b3f1459f-871d-4922-be9d-193684354eae): Will mimic replica_03
2025.09.30 15:53:18.582539 [ 323 ] {} <Warning> reports.monthly_business_metrics (b3f1459f-871d-4922-be9d-193684354eae): Log pointer of source replica replica_03 changed while we loading queue nodes. Will retry.
2025.09.30 15:54:06.790125 [ 473 ] {927d69da-9ea7-4392-a44f-46e659deb9f5} <Information> stage2.`.inner_id.20d8930f-b2dd-4a98-843e-2b25a42461a6` (f028ed82-0bd8-4219-ac1f-388eeccd414a): Became leader
2025.09.30 15:54:11.493696 [ 473 ] {6c97da7d-ff3e-410e-aa32-9e1e92bfb6fa} <Information> stage0.cdc_monitoring (19531107-99c3-436a-a1ed-4bbf4cf4fe49): It looks like the table /clickhouse/tables/01/stage0/cdc_monitoring was created by another server at the same moment, will retry
2025.09.30 15:54:11.501156 [ 473 ] {6c97da7d-ff3e-410e-aa32-9e1e92bfb6fa} <Information> stage0.cdc_monitoring (19531107-99c3-436a-a1ed-4bbf4cf4fe49): Retrying createReplica(), because some other replicas were created at the same time
2025.09.30 15:54:11.518424 [ 473 ] {6c97da7d-ff3e-410e-aa32-9e1e92bfb6fa} <Information> stage0.cdc_monitoring (19531107-99c3-436a-a1ed-4bbf4cf4fe49): Became leader
2025.09.30 15:54:11.524229 [ 314 ] {} <Information> stage0.cdc_monitoring (19531107-99c3-436a-a1ed-4bbf4cf4fe49): Replica replica_03 has log pointer '0', approximate 1 queue lag and 0 queue size
2025.09.30 15:54:11.524448 [ 314 ] {} <Information> stage0.cdc_monitoring (19531107-99c3-436a-a1ed-4bbf4cf4fe49): Not cloning replica_01, it's lost
2025.09.30 15:54:11.524546 [ 314 ] {} <Information> stage0.cdc_monitoring (19531107-99c3-436a-a1ed-4bbf4cf4fe49): Will mimic replica_03
2025.09.30 16:10:59.003531 [ 275 ] {} <Error> ServerErrorHandler: Poco::Exception. Code: 1000, e.code() = 32, I/O error: Broken pipe, Stack trace (when copying this message, always include the lines below):

0. Poco::Net::SocketImpl::error(int, String const&) @ 0x000000001498eabf
1. Poco::Net::SocketImpl::sendBytes(void const*, int, int) @ 0x000000001498fb1d
2. Poco::Net::StreamSocketImpl::sendBytes(void const*, int, int) @ 0x00000000149921f6
3. Poco::Net::HTTPSession::write(char const*, long) @ 0x000000001497d813
4. Poco::Net::HTTPHeaderIOS::~HTTPHeaderIOS() @ 0x000000001497877b
5. Poco::Net::HTTPHeaderOutputStream::~HTTPHeaderOutputStream() @ 0x0000000014978abf
6. DB::HTTPServerResponse::send() @ 0x0000000012067c88
7. DB::HTTPServerConnection::sendErrorResponse(Poco::Net::HTTPServerSession&, Poco::Net::HTTPResponse::HTTPStatus) @ 0x0000000012063d3a
8. DB::HTTPServerConnection::run() @ 0x00000000120639db
9. Poco::Net::TCPServerConnection::start() @ 0x00000000149929d2
10. Poco::Net::TCPServerDispatcher::run() @ 0x0000000014993819
11. Poco::PooledThread::run() @ 0x0000000014a8bf81
12. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000014a8a51d
13. ? @ 0x00007f5f0887e609
14. ? @ 0x00007f5f087a3353
 (version 24.3.18.7 (official build))
2025.09.30 16:18:45.620164 [ 3117 ] {} <Error> ServerErrorHandler: Poco::Exception. Code: 1000, e.code() = 32, I/O error: Broken pipe, Stack trace (when copying this message, always include the lines below):

0. Poco::Net::SocketImpl::error(int, String const&) @ 0x000000001498eabf
1. Poco::Net::SocketImpl::sendBytes(void const*, int, int) @ 0x000000001498fb1d
2. Poco::Net::StreamSocketImpl::sendBytes(void const*, int, int) @ 0x00000000149921f6
3. Poco::Net::HTTPSession::write(char const*, long) @ 0x000000001497d813
4. Poco::Net::HTTPHeaderIOS::~HTTPHeaderIOS() @ 0x000000001497877b
5. Poco::Net::HTTPHeaderOutputStream::~HTTPHeaderOutputStream() @ 0x0000000014978abf
6. DB::HTTPServerResponse::send() @ 0x0000000012067c88
7. DB::HTTPServerConnection::sendErrorResponse(Poco::Net::HTTPServerSession&, Poco::Net::HTTPResponse::HTTPStatus) @ 0x0000000012063d3a
8. DB::HTTPServerConnection::run() @ 0x00000000120639db
9. Poco::Net::TCPServerConnection::start() @ 0x00000000149929d2
10. Poco::Net::TCPServerDispatcher::run() @ 0x0000000014993819
11. Poco::PooledThread::run() @ 0x0000000014a8bf81
12. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000014a8a51d
13. ? @ 0x00007f5f0887e609
14. ? @ 0x00007f5f087a3353
 (version 24.3.18.7 (official build))
